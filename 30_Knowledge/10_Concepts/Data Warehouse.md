
---

## **The Big Idea**

  

A **Data Warehouse (DWH)** is a central system that stores **clean, structured, and analytics-ready data** from multiple sources.

Itâ€™s optimized for **reading, aggregating, and analyzing** â€” not for frequent updates like a regular database.

---

## **What Makes It Special?**

|**Feature**|**Description**|
|---|---|
|Read-optimized|Fast for SELECT, slow for frequent UPDATE/DELETE|
|Historical|Stores **years** of data, often **append-only**|
|Structured|Follows a **schema** (tables, types, relationships)|
|Analytical|Built for **OLAP**, not OLTP|

See also: [[OLAP vs OLTP]]

---

## **Common Use Cases**

- Business dashboards and KPIs
    
- Financial or sales reports
    
- Machine learning model training
    
- Long-term data storage for compliance
    
- Joining data from multiple systems
    

---

## **Typical Workflow**

```
 [Source Systems] â†’ [ETL/ELT] â†’ [Staging] â†’ [DWH] â†’ [BI/ML/Apps]
      CRMs, APIs     Airflow, dbt        tables       Looker, Tableau, Python
```

The DWH is the **hub** for clean data.

---

## **Warehouse vs Database**

|**Feature**|**Data Warehouse**|**Transactional Database**|
|---|---|---|
|Optimized for|Aggregation, reporting (OLAP)|Fast inserts, updates (OLTP)|
|Schema design|Star/snowflake (dim/fact)|Normalized (3NF)|
|Writes|Batched (daily/hourly loads)|Constant (per user action)|
|Example tools|Snowflake, BigQuery, Redshift|MySQL, PostgreSQL, MongoDB|
|Data size|TBâ€“PB range|MBâ€“GB range|

---

## **Examples of Warehouses**

|**Product**|**Type**|**Notes**|
|---|---|---|
|**Snowflake**|Cloud-native|Scales compute/storage separately, SQL-based|
|**BigQuery**|Serverless|Google Cloudâ€™s DWH, very fast on big scans|
|**Redshift**|Cluster-based|AWS-native, older than Snowflake/BigQuery|
|**Databricks**|Lakehouse|Combines DWH + Data Lake (via Delta Lake)|
|**ClickHouse**|Open-source|Super-fast analytics DB (used in Yandex)|

---

## **Data Modeling Inside a DWH**

  

Typical structure uses **dimensional modeling**:

- **Fact tables** â€” numbers you want to measure (sales, clicks)
    
- **Dimension tables** â€” context for the numbers (date, customer, product)
    

  

Example:

```
Fact: sales â†’ amount, date_id, product_id, store_id  
Dims: date, product, store
```

This is the **star schema** â€” see: [[Dimensional Modeling (Kimball)]]

---

## **Partitioning and Clustering**

  

To improve performance and cost, DWHs use:

- **Partitioning** â€” split data by date, region, etc.
    
- **Clustering** â€” organize data inside partitions for fast filtering
    

  

> Good partitioning = cheaper queries

---

## **Storage Format**

  

Many DWHs store data in **columnar** formats (e.g., Parquet) for:

- Faster aggregation (only read the needed columns)
    
- Better compression (columns are more similar)
    
- Predicate pushdown (skip irrelevant data blocks)
    

  

See: [[File Formats: CSV, JSON, Parquet, Avro]]

---

## **Warehouse Layers (Bronze â†’ Silver â†’ Gold)**

  

A typical DWH follows a **layered architecture**:

|**Layer**|**Purpose**|
|---|---|
|Bronze|Raw ingested data|
|Silver|Cleaned and typed tables|
|Gold|Final, business-ready models|

This makes the pipeline **modular, testable, and traceable**.

  

See: [[Data Lakehouse: Bronzeâ€“Silverâ€“Gold]]

---

## **Cost Considerations**

  

Warehouses charge for:

- **Storage** (GBs per month)
    
- **Compute** (query time or slots)
    
- **Data scans** (especially in BigQuery)
    

  

ğŸ’¡ Tips:

- Use SELECT column1, column2 â€” **not SELECT ***
    
- Partition large tables by date or ID
    
- Materialize heavy queries if reused often
    

---

## **Observability and Governance**

  

Important for trust and compliance:

- **Data lineage** â€” where data came from
    
- **Access control** â€” role-based permissions
    
- **Auditing** â€” who queried what, when
    
- **Documentation** â€” table and column descriptions
    

  

Tools: dbt docs, Amundsen, DataHub, Monte Carlo, OpenLineage

---

## **Common Pitfalls**

- âŒ Loading dirty data â€” always clean first
    
- âŒ No schema enforcement â€” adds risk
    
- âŒ No incremental strategy â€” slow and expensive loads
    
- âŒ No freshness monitoring â€” reports break silently
    
- âŒ Poor naming â€” causes confusion across teams
    

---

## **Quick Checklist**

- Tables are partitioned and clustered
    
- Gold layer is tested and documented
    
- Raw data preserved (audit/compliance)
    
- Queries are optimized (no SELECT *)
    
- Access and roles reviewed regularly
    
- Lineage and ownership are known
    

---

**Related notes:**

[[ETL]] Â· [[ELT vs ETL]] Â· [[Dimensional Modeling (Kimball)]] Â· [[Batch vs Streaming]] Â· [[Data Lakehouse: Bronzeâ€“Silverâ€“Gold]] Â· [[Data Pipeline]]

---
