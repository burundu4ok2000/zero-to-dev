![[DoD.png]]

## **Что такое DoD «по-человечески»**

  
**Definition of Done** — это список конкретных критериев «как понять, что навык действительно усвоен и даёт результат». Когда все пункты DoD закрыты — можно считать, что уровень по этому навыку «сдан». В Scrum это инструмент прозрачности: общий стандарт качества, по которому видно, что работа действительно **готова/готова к показу**. 

---

## **Шаблон для Obsidian**

  

Скопируй и заполняй под себя (каждый пункт — наблюдаемое доказательство):

```
### <Навык — Уровень>
- [ ] Я умею: <кратко, простыми словами>
- [ ] Применил(а) это в мини-проекте/кейсе
- [ ] Замерил(а) эффект (время, стоимость, корректность)
- [ ] Evidence: ссылка на репо/ноут/видео (3–7 мин)
```

### <Навык — Уровень>
- [ ] Я умею: <кратко, простыми словами>
- [ ] Применил(а) это в мини-проекте/кейсе
- [ ] Замерил(а) эффект (время, стоимость, корректность)
- [ ] Evidence: ссылка на репо/ноут/видео (3–7 мин)

---

## **Примеры DoD по ключевым навыкам**

  

### **SQL — intermediate**

- Пишу **оконные функции** (ROW_NUMBER, LAG/LEAD, оконные SUM/AVG) для расчётов «по окну», а не через подзапросы. 
    
- Уверенно делаю **сложные JOIN** (многие-ко-многим, анти-/semi-join логика) без дублирования строк.
    
- Умею читать **план выполнения** (EXPLAIN/EXPLAIN ANALYZE), отличаю seq scan от index scan, понимаю, какой алгоритм JOIN использован. 
    
- Решены **2 дата-кейса** в репо + README с объяснением, где и зачем применены окна и как оптимизировал(а) запросы.
    
- Evidence: ссылки на SQL-файлы, скрин плана выполнения и краткое видео (до 5 мин), где проговариваю логику.
    
    _(Подсказка:_ _EXPLAIN ANALYZE_ _показывает фактическое время/кол-во строк по шагам плана — незаменимо для профилирования.)_ 
    

  

### **Python —** **Working**

- Маленький **ETL-скрипт**: читаю API/CSV, обрабатываю, пишу в БД; есть логирование и обработка ошибок.
    
- Код упакован как **модуль/CLI**, структура папок понятна, зависимости закреплены (requirements.txt).
    
- Evidence: репо с README (как запустить, пример конфигов), короткое демо-видео.
    

  

### **Git / Linux / Docker —** **Working**

- Ветка → PR → merge, умею делать реверт, разбираю конфликт.
    
- Уверенно пользуюсь shell-утилитами (grep, awk/sed, find, permissions).
    
- Собираю **Dockerfile**, запускаю контейнер/compose, монтирую тома.
    
- Evidence: репо с Dockerfile и docker-compose.yml, скрины git log --graph.
    

  

### **Моделирование & DWH (dimensional) —** **Working**

- Построил(а) слой **staging → marts** (звёздочная схема: факты + измерения).
    
- Объясняю, где партиционировать/кластеризовать и зачем.
    
- Evidence: диаграмма, SQL-модели, заметка с объяснением выбора ключей/гранулярности.
    

  

### **dbt** **—** **Working**

- Прошёл(ла) **dbt Quickstart** под свой DWH.
    
- Настроены **тесты** (not null / unique / relationships) и **документация** (dbt docs generate/serve). 
    
- Есть хотя бы одна **incremental-модель** с описанной стратегией апдейта.
    
- Evidence: репо с models/, скрин каталога docs, README с картинкой lineage.
    

  

### **Оркестрация (** Aitflow) - Working

- Есть **DAG** с расписанием, ретраями, логами; использую сенсор/зависимости.
    
- Понимаю разницу между **DAG** и задачей, что делает **scheduler**. 
    
- Evidence: скрин графа DAG, код dags/*.py, пару слов о политике ретраев/идемпотентности.
    
    _(Можно использовать пресеты расписания, например_ _@hourly__.)_ 
    

  

### **Стриминг (** **Kafka → Spark Structured Streaming** **) —**  **Working**

- Поднял(а) локально Kafka/Confluent Cloud, сделал **producer/consumer**. Понимаю «топик/партиция/группа». 
    
- Написал(а) **Spark Structured Streaming** джобу с окнами, watermark и checkpointing; объясняю, чем потоковый расчёт отличается от batch. 
    
- Evidence: репо (код producer/consumer + Spark job), README с **latency/throughput**.
    

  

### **Тестирование данных (** **Great Expectations** **) —**  **Working**

- Создан проект GX, есть **Expectation Suites** (схема/полнота/диапазоны).
    
- Включены **Data Docs** (HTML-отчёты) и ссылка на них прикреплена к PR/релизу. 
    
- Evidence: папка great_expectations/ + скрин Data Docs.
    

  

### **Облако (GCP / Azure) —**  **Core**

- Собран **минимальный контур**: хранилище + DWH, сервисный аккаунт/IAM, секреты, мониторинг.
    
- Evidence: диаграмма, Terraform (по желанию), скрин из консоли.
    

  

### **CI/CD —**  **Core**

- **GitHub Actions**/GitLab CI: шаги lint/test/build, артефакт, и (по необходимости) deploy.
    
- Evidence: .github/workflows/*.yml, скрин «зелёного» прогона.
    

---

### **Как этим пользоваться**

1. Выбери 2–3 навыка на квартал.
    
2. Скопируй шаблон и преврати пункты DoD в чек-лист **в твоём репо/заметке**.
    
3. По закрытии чек-листов добавь **Evidence** (репо/скрин/видео). Это и есть «доказательство готовности» — как в DoD из Scrum. 
    
