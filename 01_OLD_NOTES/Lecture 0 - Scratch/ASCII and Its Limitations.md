## ![[Screenshot 2025-06-23 at 18.45.05.png]]
**Historical Limitation**

  

In the early days of computing, the **ASCII (American Standard Code for Information Interchange)** system allocated only **7 bits** per character. This allowed for just **128 possible characters** (2‚Å∑ = 128), which was enough for:




- English letters (uppercase and lowercase)
    
- Numbers (0‚Äì9)
    
- Basic punctuation and control characters
    

  

However, it left no room for:

- Non-English letters or accented characters
    
- Symbols from other alphabets
    
- Emojis or extended symbols
    

P.S.  You can pronounce the expression **(2‚Å∑ = 128)** in English as:

  

>  ‚Äú**Two raised to the power of seven is one hundred twenty-eight.**‚Äù

You can pronounce **‚ÄúNumbers (0‚Äì9)‚Äù** in English as:

  

> **‚ÄúNumbers from zero to nine.‚Äù**

  

Or slightly more formally:

  

> **‚ÄúThe digits zero through nine.‚Äù**

## **The Expansion to 8 Bits**

  

Eventually, ASCII was extended to use **8 bits**, allowing for **256 characters**. This extra bit allowed room for:

- Additional symbols
    
- Simple graphics
    
- Some localized language characters (e.g., Western European letters)
    

  

Still, 256 was not enough for global communication needs.

  

## **Why ‚ÄúUnfortunately‚Äù?**

  

David Malan (CS50x) refers to this 7-bit limitation as ‚Äúunfortunate‚Äù because:

- It constrained early software and data formats
    
- It led to fragmented systems of extended ASCII
    
- It delayed the adoption of more inclusive encodings
    

  

## **Modern Solution**

  

To solve these limitations, modern systems use **Unicode**, especially **UTF-8**, which supports **over 1 million characters**, including:

- Characters from all modern and historical writing systems
    
- Emojis üòÑ
    
- Mathematical and technical symbols
    

  

## **Summary**

  

The original 7-bit ASCII was a milestone‚Äîbut its limited capacity required evolution. The shift to UTF-8 and Unicode reflects a more inclusive and globally compatible approach to text representation in computing.

[[CS50x Harvard]]
[[10_Courses/CS50x Harvard/00_Lectures/Lecture 0 - Scratch]]
